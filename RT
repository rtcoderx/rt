#Information Visualisation for text data - exp 8
import matplotlib.pyplot as plt
from collections import Counter
def count_words(text):
    words = text.split()
    word_count = len(words)
    return word_count
def visualize_word_count(text):
    words = text.split()
    word_counts = Counter(words)
    plt.bar(word_counts.keys(), word_counts.values())
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.title('Word Count Visualization')
    plt.show()
def calculate_word_proportion(text):
    words = text.split()
    word_counts = Counter(words)
    total_words = len(words)
    word_proportions = {word: count / total_words for word, count in word_counts.items()}
    return word_proportions
# Example usage with user input:
text_data = input("Enter your text: ")
word_count = count_words(text_data)
print(f'Total words: {word_count}')
visualize_word_count(text_data)
word_proportions = calculate_word_proportion(text_data)
print(f'Word Proportions: {word_proportions}')


#Information Extraction - Exp 9
import spacy
nlp = spacy.load("en_core_web_sm")
text = """
Apple Inc. is an American multinational technology company headquartered in Cupertino, California.  It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.
Apple designs, manufactures, and markets consumer electronics, computer software, and online services.
"""
doc = nlp(text)
for entity in doc.ents:
    print(f"Entity: {entity.text}, Label: {entity.label_}")
for token in doc:
    if token.text.lower() == "founded" and "ORG" in [child.ent_type_ for child in token.head.children]:
        organization = [ent.text for ent in token.head.children if ent.ent_type_ == "ORG"]
        if organization:
            print(f"{organization[0]} was founded by:")
            founders = [child.text for child in token.children if child.ent_type_ == "PERSON"]
            for founder in founders:
                print(founder)



#Sentiment Analysis - Exp 4,Exp 10,11
import nltk
from nltk.corpus import movie_reviews
import random
nltk.download('movie_reviews')
documents = [(list(movie_reviews.words(fileid)), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]
random.shuffle(documents)
all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]
def document_features(document):
    return {'contains({})'.format(word): (word in set(document)) for word in word_features}
featuresets = [(document_features(d), c) for (d, c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = nltk.NaiveBayesClassifier.train(train_set)
accuracy = nltk.classify.accuracy(classifier, test_set)
print(f"Accuracy: {accuracy:.2%}")
classifier.show_most_informative_features(5)



#TOPIC DETETCTION -1,2,3
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from nltk.corpus import stopwords
import numpy as np
corpus = [
    "Rafael Nadal Joins Roger Federer in Missing U.S. Open",
    "Rafael Nadal Is Out of the Australian Open",
    "Biden Announces Virus Measures",
    "Biden's Virus Plans Meet Reality",
    "Where Biden's Virus Plan Stands"
]
count_vect = CountVectorizer(stop_words=stopwords.words('english'), lowercase=True)
x_counts = count_vect.fit_transform(corpus)
x_tfidf = TfidfTransformer().fit_transform(x_counts)
lda = LDA(n_components=2)
lda_array = lda.fit_transform(x_tfidf)
features = count_vect.get_feature_names_out()
important_words = [
    [features[i] for i in np.argsort(lda.components_[j])[::-1][:3]]
    for j in range(lda.n_components)
]
print(important_words)




#Facebook data classification - exp 12
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
df = pd.read_csv('your_facebook_data.csv')
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['text'])
X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)
classifier = MultinomialNB()
classifier.fit(X_train, y_train)
predictions = classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)
print(f'Accuracy: {accuracy:.2f}')
print('Classification Report:\n', report)


# text classification -1
import pandas as pd
data = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding='latin-1')
data.head()
data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)
data.columns = ['label', 'text']
data.head()
data.isna().sum()
data.shape
data['label'].value_counts(normalize = True).plot.bar()
import nltk
nltk.download('all')
text = list(data['text'])
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
corpus = []
for i in range(len(text)):
    r = re.sub('[^a-zA-Z]', ' ', text[i])
    r = r.lower()
    r = r.split()
    r = [word for word in r if word not in stopwords.words('english')]
    r = [lemmatizer.lemmatize(word) for word in r]
    r = ' '.join(r)
    corpus.append(r)
data['text'] = corpus
data.head()
X = data['text']
y = data['label']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)
print('Training Data :', X_train.shape)
print('Testing Data : ', X_test.shape)
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X_train_cv = cv.fit_transform(X_train)
X_train_cv.shape
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train_cv, y_train)
X_test_cv = cv.transform(X_test)
predictions = lr.predict(X_test_cv)
predictions
# confusion matrix
import pandas as pd
from sklearn import metrics
df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])
df


